from airflow.decorators import task
from airflow.hooks.S3_hook import S3Hook

import pandas as pd


@task
def Model_NB():
    print("NB Model")
    return {
        "model": "Naive Bayes",
        "precision": 0.5,
        "recall": 0.5,
    }


@task
def Model_RF(**kwargs):
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GridSearchCV
    import numpy as np

    ti = kwargs["ti"]
    # Get key from the PrepareData task
    X_train_key = ti.xcom_pull(task_ids="PrepareData", key="X_train")
    Y_train_key = ti.xcom_pull(task_ids="PrepareData", key="y_train")
    X_test_key = ti.xcom_pull(task_ids="PrepareData", key="X_test")

    # Download from S3
    s3 = S3Hook(aws_conn_id="custom_s3")
    X_train_path = s3.download_file(key=X_train_key, bucket_name="dag-umba")
    Y_train_path = s3.download_file(key=Y_train_key, bucket_name="dag-umba")
    X_test_path = s3.download_file(key=X_test_key, bucket_name="dag-umba")

    # Load Numpy arrays
    X_train = np.load(X_train_path)
    y_train = np.load(Y_train_path)
    X_test = np.load(X_test_path)

    # Seting the Hyper Parameters
    param_grid = {
        "max_depth": [3, 5, 7, 10, None],
        "n_estimators": [3, 5, 10, 25, 50, 150],
        "max_features": [4, 7, 15, 20],
    }

    # Creating the classifier
    model = RandomForestClassifier(random_state=2)

    grid_search = GridSearchCV(
        model, param_grid=param_grid, cv=5, scoring="recall", verbose=4
    )
    grid_search.fit(X_train, y_train)

    rf = RandomForestClassifier(
        max_depth=grid_search.best_params_["max_depth"],
        max_features=grid_search.best_params_["max_features"],
        n_estimators=grid_search.best_params_["n_estimators"],
        random_state=2,
    )
    # trainning with the best params
    rf.fit(X_train, y_train)

    # Testing the model
    # Predicting using our  model
    y_pred = rf.predict(X_test)

    ds = kwargs["execution_date"].strftime("%Y-%m-%d-%H-%M-%S")
    key = ds + "/trainingData/y_pred.npy"
    np.save("y_pred.npy", y_pred)
    s3.load_file(
        filename="y_pred.npy",
        key=key,
        bucket_name="dag-umba",
        replace=True,
    )
    # with open("archivoprueba2.txt", "a", encoding="utf-8") as f:
    #     f.write("Best Score generated by a Grid \n ")
    #     f.write(str(grid_search.best_score_))
    #     f.write("\n Parameters of the Best calculated Grid \n ")
    #     print(grid_search.best_params_, file=f)
    #     f.write("\n \n Accuracy Score \n ")
    #     f.write(str(accuracy_score(y_test, y_pred)))
    #     f.write("\n \n confusion matrix \n ")
    #     f.write(str(confusion_matrix(y_test, y_pred)))
    #     f.write("\n \n beta score \n ")
    #     f.write(str(fbeta_score(y_test, y_pred, beta=2)))

    # df = pd.DataFrame(y_pred, columns=["Prediction"])
    # df.to_csv(r"Paths3/RandomForest", index=False)

    return {
        "return_value": key,
        "details": "Best Score generated by Grid Search: {}\nParameters of the Best calculated Grid: {}".format(
            grid_search.best_score_, grid_search.best_params_
        ),
    }


@task
def Model_XGB():
    print("XGB Model")
    return {
        "model": "Naive Bayes",
        "precision": 0.5,
        "recall": 0.5,
    }


@task
def Custom_Pipeline(dataPath):
    s3 = S3Hook(aws_conn_id="custom_s3")
    downloadPath = s3.download_file(key=dataPath, bucket_name="dag-umba")
    df_credit = pd.read_csv(downloadPath, index_col=0)
    print("Pipeline Model")
    return {
        "model": "Custom Pipeline",
        "precision": 0.5,
        "recall": 0.5,
    }
